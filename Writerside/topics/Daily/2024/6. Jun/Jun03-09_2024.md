# June 2024, Week 2: June 3rd - June 9th

## June 3 -> 2486. Append Characters to String to Make Subsequence {collapsible="true" default-state="collapsed"}

You are given two strings `s` and `t` consisting of only lowercase English letters.

Return *the minimum number of characters that need to be appended to the end of* `s` *so that* `t` *becomes
a **subsequence** of* `s`.

A **subsequence** is a string that can be derived from another string by deleting some or no characters without changing
the order of the remaining characters.

**Example 1:**

- **Input:** `s = "coaching"`, `t = "coding"`
- **Output:** 4
- **Explanation:** Append the characters "ding" to the end of `s` so that `s = "coachingding"`.
  Now, `t` is a subsequence of `s` (`"coachingding"`).
  It can be shown that appending any three characters to the end of `s` will never make `t` a subsequence.

**Example 2:**

- **Input:** `s = "abcde"`, `t = "a"`
- **Output:** 0
- **Explanation:** `t` is already a subsequence of `s` (`"abcde"`).

**Example 3:**

- **Input:** `s = "z"`, `t = "abcde"`
- **Output:** 5
- **Explanation:** Append the characters `"abcde"` to the end of `s` so that `s = "zabcde"`.
  Now, `t` is a subsequence of `s` (`"zabcde"`).
  It can be shown that appending any four characters to the end of `s` will never make `t` a subsequence.

**Constraints:**

- `1 <= s.length, t.length <= 10^5`
- `s` and `t` consist only of lowercase English letters.

---

### Approach 1: Two Pointer Technique {id="approach_d1_1" collapsible="true" default-state="expanded"}

```Python
def appendCharacters1(s: str, t: str) -> int:
    """
    Calculates the minimum number of characters from string 't' 
    that must be appended to string 's' to make 't' a subsequence of 's'.

    This function iterates through both strings, comparing characters at
    corresponding positions.  When a match is found, it advances in both
    strings; otherwise, it only moves forward in the first string. The
    result is the number of characters remaining in 't' after the comparison,
    indicating how many need to be appended.

    The function operates in O(n) time complexity, where 'n' is the length of
    the longer string since each character of the string 's' and 't' is
    visited at most once. The space complexity is O(1) as the solution here
    does not require additional space that scales with input size.
    """
    s_index = 0
    t_index = 0

    s_length = len(s)
    t_length = len(t)

    while s_index < s_length and t_index < t_length:
        if s[s_index] == t[t_index]:
            t_index += 1
        s_index += 1

    # Return the count of remaining characters in 't' that needs to be
    # appended to 's'
    return t_length - t_index
```

{collapsible="true" default-state="expanded" collapsed-title="Two Pointer Technique Code..."}

#### Understanding the Core Idea {id="core-idea_d1_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage a two-pointer technique to efficiently compare and traverse both
strings simultaneously.
This approach allows us to determine how much of string `t` is already a subsequence of `s`, and
consequently, how many characters from `t` need to be appended to `s`.

- **Subsequence Matching:** The solution exploits the fact that we only need to find characters of `t` in `s` in the
  same order, not necessarily consecutively.
- **Efficient Traversal:** By using two pointers, we can avoid unnecessary comparisons and achieve linear time
  complexity.

> **Key Insight:**
> The number of characters to append is simply the number of characters in `t` that weren't matched in `s`,
> which can be calculated as the difference between the length of `t` and the final position of the `t` pointer.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d1_1" collapsible="true" default-state="expanded"}

1. **Initialization:**
   ```python
   s_index = 0
   t_index = 0

   s_length = len(s)
   t_length = len(t)
   ```
   We initialize two pointers, `s_index` and `t_index`, to keep track of our current positions in strings `s` and `t`
   respectively.
   We also store the lengths of both strings for easier access and improved readability.

2. **Main Comparison Loop:**
   ```python
   while s_index < s_length and t_index < t_length:
       if s[s_index] == t[t_index]:
           t_index += 1
       s_index += 1
   ```
   This is the core of the algorithm. We iterate through both strings simultaneously:
    - If the characters at the current positions match, we move forward in both strings (incrementing both `s_index`
      and `t_index`).
    - If they don't match, we only move forward in `s` (incrementing only `s_index`).
    - The loop continues until we reach the end of either string.

   This approach ensures that we find the longest subsequence of `t` that exists in `s`.

3. **Result Calculation:**
   ```python
   return t_length - t_index
   ```
   After the loop, `t_index` represents how many characters of `t` we successfully matched in `s`.
   Therefore, `t_length - t_index` gives us the number of remaining characters in `t` that need to be appended to `s`.

---

#### Complexity Analysis {id="complexity-analysis_d1_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n)$, where $n$ is the length of the longer string between `s` and `t`.
  This is because we iterate through each character of `s` at most once, and each character of `t` at most once.
  The worst-case scenario occurs when `s` and `t` have no matching characters,
  causing us to traverse the entire length of `s`.

**Space Complexity:**

- $O(1)$, or constant space. The algorithm uses a fixed number of
  variables (`s_index`, `t_index`, `s_length`, `t_length`) regardless of the input size. It doesn't require any
  additional data structures that grow with the input size.

---

#### Example {id="example_d1_1" collapsible="true" default-state="expanded"}

**Input:**

```Python
s = "coaching"
t = "coding"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - `s_index` (pointer for `s`) is set to 0.
    - `t_index` (pointer for `t`) is set to 0.
    - `s_length` is calculated as 8 (length of "coaching").
    - `t_length` is calculated as 6 (length of "coding").

2. **Main Loop (Comparing 's' and 't'):**
    - **Iteration 1:**
        - **Comparison:** `s[0]` ('c') matches `t[0]` ('c').
        - **Action:**  Both `s_index` and `t_index` are incremented to 1.

    - **Iteration 2:**
        - **Comparison:** `s[1]` ('o') matches `t[1]` ('o').
        - **Action:** Both `s_index` and `t_index` are incremented to 2.

    - **Iterations 3 - 8:**
        - **Comparison:** In each iteration, `s[s_index]` does not match `t[t_index]` ('d').
        - **Action:** Only `s_index` is incremented.
        - **Note:** We're essentially skipping over characters in `s` ("aching") that are not part of the subsequence
          we're looking for ("coding").

3. **Loop Termination:**
    - The loop terminates because `s_index` reaches 8 (end of `s`).
    - At this point, `t_index` is still at 2.

4. **Iteration Summary:**

   | `s_index` | `t_index` | `s[s_index]` | `t[t_index]` | Match? |
   |-----------|-----------|--------------|--------------|--------|
   | 0         | 0         | c            | c            | Yes    |
   | 1         | 1         | o            | o            | Yes    |
   | 2         | 2         | a            | d            | No     |
   | 3         | 2         | c            | d            | No     |
   | 4         | 2         | h            | d            | No     |
   | 5         | 2         | i            | d            | No     |
   | 6         | 2         | n            | d            | No     |
   | 7         | 2         | g            | d            | No     |

5. **Result Calculation:**
    - `t_length - t_index` = 6 - 2 = 4.
    - This indicates that `4` characters ("ding") need to be appended to `s` to make `t` a subsequence.

---

## June 4 -> 409. Longest Palindrome {collapsible="true" default-state="collapsed"}

Given a string `s` which consists of lowercase or uppercase letters, return the length of the **longest palindrome**
that can be built with those letters.

Letters are **case-sensitive**, for example, `"Aa"` is not considered a palindrome.

**Example 1:**

- **Input:** `s = "abccccdd"`
- **Output:** 7
- **Explanation:** One longest palindrome that can be built is `"dccaccd"`, whose length is 7.

**Example 2:**

- **Input:** `s = "a"`
- **Output:** 1
- **Explanation:** The longest palindrome that can be built is `a`, whose length is 1.

**Constraints:**

- `1 <= s.length <= 2000`
- `s` consists of lowercase **and/or** uppercase English letters only.

---

### Approach 1: Frequency Counting with Dictionary {id="approach_d2_1" collapsible="true" default-state="expanded"}

```Python
def longestPalindrome1(s: str) -> int:
    """
    Calculates the length of the longest palindrome that can be built with
    the characters in the input string 's'.

    First, the function counts the frequency of each character using a
    defaultdict 'char_count'. Then, it iterates through the counts and if
    the count is even, it adds the count to the result. If the count is odd,
    it adds one less than the count to the result and sets 'odd_exists' flag
    to True. This is done because palindromes can have at most one character
    with an odd count (at the center of the palindrome); all other characters
    must occur an even number of times. Finally, if there was at least one
    character with an odd count, it adds 1 to the result, accounting for the
    possible center element in the palindrome.

    The time complexity of this function is O(n), where 'n' is the length of
    the input string 's'. This is because the function iterates over the
    string 's' once to count characters and iterates over every character
    frequency in 'char_count' once. The space complexity of this function is
    O(1) because the 'char_count' dictionary will at most contain entries
    equal to the number of different characters which are constant.
    """
    char_count = defaultdict(int)

    for char in s:
        char_count[char] += 1

    result = 0
    odd_exists = False

    for _, count in char_count.items():
        if count % 2 == 0:
            result += count
        else:
            result += count - 1
            odd_exists = True

    # If there was at least one character with an odd count, it can be used
    # as the center of the palindrome
    if odd_exists:
        result += 1

    return result
```

{collapsible="true" default-state="expanded" collapsed-title="Frequency Counting with Dictionary Code..."}

#### Understanding the Core Idea {id="core-idea_d2_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage a frequency counting technique using a dictionary to determine the
maximum length of a palindrome that can be constructed from the given string.
This approach exploits two key properties of palindromes:

- **Even-count Characters:** Characters that appear an even number of times can be fully used in a palindrome, with
  half on each side.
- **Odd-count Characters:** For characters that appear an odd number of times, we can use all but one occurrence in
  pairs, and potentially use one leftover character as the center of the palindrome.

> **Key Insight:** The solution cleverly handles odd-count characters by subtracting 1 from their count and flagging
> that an odd count exists.
> This allows for the inclusion of one odd-count character as the center of the palindrome if needed.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d2_1" collapsible="true" default-state="expanded"}

1. **Initialization:**
   ```python
   char_count = defaultdict(int)
   ```
   We initialize a `defaultdict` to store the frequency of each character.
   Using `defaultdict(int)` allows us to increment counts without explicitly checking if a key exists.

2. **Character Counting:**
   ```python
   for char in s:
       char_count[char] += 1
   ```
   This loop iterates through each character in the input string `s`, incrementing its count in the `char_count`
   dictionary.

3. **Palindrome Length Calculation:**
   ```python
   result = 0
   odd_exists = False

   for _, count in char_count.items():
       if count % 2 == 0:
           result += count
       else:
           result += count - 1
           odd_exists = True
   ```
   Here, we iterate through the character counts:
    - For even counts, we add the full count to the result.
    - For odd counts, we add one less than the count (making it even) and set `odd_exists` to `True`.

4. **Handling Center Character:**
   ```python
   if odd_exists:
       result += 1
   ```
   If we encountered any odd-count characters, we can use one as the center of the palindrome, so we add 1 to the
   result.

5. **Return Result:**
   ```python
   return result
   ```
   The function returns the calculated length of the longest possible palindrome.

---

#### Complexity Analysis {id="complexity-analysis_d2_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n)$, where $n$ is the length of the input string `s`.
  This is because we iterate through the string once to count characters, and then iterate through the character counts.
  Since there's a fixed number of possible characters, the second iteration is bounded by a constant.

**Space Complexity:**

- $O(1)$, because the `char_count` dictionary will contain at most 52 entries (26 lowercase and 26 uppercase letters),
  which is a constant regardless of the input size.

---

#### Example {id="example_d2_1" collapsible="true" default-state="expanded"}

**Input:**

```Python
s = "abccccdd"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - `char_count` is initialized as a defaultdict: `char_count = defaultdict(int)`
    - `result` is initialized to 0: `result = 0`
    - `odd_exists` is initialized to False: `odd_exists = False`

2. **Character Frequency Counting:**
    - The loop iterates through each character in the string `s = "abccccdd"`.
    - `char_count` is updated as follows:
        - `{'a': 1}` (after processing 'a')
        - `{'a': 1, 'b': 1}` (after processing 'b')
        - `{'a': 1, 'b': 1, 'c': 1}` ... and so on until:
        - `{'a': 1, 'b': 1, 'c': 4, 'd': 2}` (after processing all characters)

3. **Palindrome Length Calculation:**
    - The loop iterates over the `char_count` dictionary:
        - `'a'`: Count is 1 (odd), so 0 is added to `result` (result remains 0), and `odd_exists` is set to `True`.
        - `'b'`: Count is 1 (odd), so 0 is added to `result` (result remains 0), and `odd_exists` remains `True`.
        - `'c'`: Count is 4 (even), so 4 is added to `result` (result becomes 4).
        - `'d'`: Count is 2 (even), so 2 is added to `result` (result becomes 6).

4. **Center Character Adjustment:**
    - Since `odd_exists` is `True` (we found odd-frequency characters), 1 is added to `result`. The final `result` is 7.

5. **Result Calculation/Return:**
    - The function returns the final value of `result`, which is 7. This indicates that the longest possible palindrome
      we can construct from the letters in `"abccccdd"` has a length of 7 (e.g., `"dccaccd"`).

---

### Approach 2:Set-based Character Pairing {id="approach_d2_2" collapsible="true" default-state="expanded"}

```Python
def longestPalindrome2(s: str) -> int:
    """
    Calculates the length of the longest palindrome that can be built with
    the characters in the input string 's'.

    This function uses a set `character_set` to keep track of characters
    encountered. For each character, if it is already in the set, it can be
    paired with its existing counterpart, contributing 2 to the palindrome
    length. If not in the set, it is added to the set as it may be paired
    with a future character. In the end, if `character_set` still contains
    characters, it means a palindrome can still fit one more character in
    its middle. Therefore, the result is incremented by 1 if `character_set`
    is not empty.

    The time complexity is O(n), where `n` is the length of the input string
    due to the single pass through the string. The space complexity is O(1)
    since the set will contain at most 52 characters (26 lowercase and 26
    uppercase).
    """
    character_set = set()
    result = 0

    for char in s:
        if char in character_set:
            result += 2
            character_set.remove(char)
        else:
            character_set.add(char)

    # If there are characters left in the set, one of them can be used as the
    # center of the palindrome
    if character_set:
        result += 1

    return result
```

{collapsible="true" default-state="expanded" collapsed-title="Set-based Character Pairing Code..."}

#### Understanding the Core Idea {id="core-idea_d2_2" collapsible="true" default-state="expanded"}

The central concept of this solution is to use a set data structure to efficiently pair up characters for forming a
palindrome.
This approach leverages the following key ideas:

- **Character Pairing:** Each time we encounter a character that's already in the set, we can form a pair, contributing
  2 to the palindrome length.
- **Unpaired Characters:** Characters that don't find a pair remain in the set, and one of them can potentially be used
  as the center of the palindrome.

> **Key Insight:**
> By using a set, we can achieve $O(1)$ lookup and removal operations, making the pairing process very
> efficient.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d2_2" collapsible="true" default-state="expanded"}

1. **Initialization:**
   ```python
   character_set = set()
   result = 0
   ```
   We initialize an empty set to keep track of unpaired characters and a variable to store the palindrome length.

2. **Character Pairing Process:**
   ```python
   for char in s:
       if char in character_set:
           result += 2
           character_set.remove(char)
       else:
           character_set.add(char)
   ```
   This loop iterates through each character in the input string `s`:
    - If the character is already in the set, we've found a pair.
      We increment `result` by 2 and remove the character from the set.
    - If the character is not in the set, we add it to the set as a potential future pair.

3. **Handling Center Character:**
   ```python
   if character_set:
       result += 1
   ```
   After processing all characters, if the set is not empty, it means we have unpaired characters.
   We can use one of these as the center of the palindrome, so we add 1 to the result.

4. **Return Result:**
   ```python
   return result
   ```
   The function returns the calculated length of the longest possible palindrome.

---

#### Complexity Analysis {id="complexity-analysis_d2_2" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n)$, where $n$ is the length of the input string `s`.
  We iterate through the string once, performing $O(1)$ operations (set lookup, addition, and removal)
  for each character.

**Space Complexity:**

- $O(1)$, because the `character_set` will contain at most 52 unique characters (26 lowercase and 26 uppercase letters),
  which is a constant regardless of the input size.

---

#### Example {id="example_d2_2" collapsible="true" default-state="expanded"}

**Input:**

```Python
s = "abccccdd"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - An empty set `character_set` is created.
    - `result` is set to 0: `result = 0`

2. **Character Processing Loop:**
    - Iteration 1: The character 'a' is not in `character_set`, so it's added to the set.
    - Iteration 2: The character 'b' is not in `character_set`, so it's added to the set.
    - Iteration 3 & 4: The character 'c' is first added to the set.
      In the next iteration, 'c' is found in the set, so it's removed, and `result` is incremented by 2.
    - Iteration 5 & 6: The character 'c' is again added and then removed from the set, incrementing `result` by 2 again.
    - Iteration 7 & 8: The character 'd' follows the same add-remove pattern as 'c,' increasing `result` by another 2.

3. **Iteration Summary (Palindrome Length Calculation):**

   | Iteration | Character | Action                  | Character Set   | Result |
   |-----------|-----------|-------------------------|-----------------|--------|
   | 1         | a         | Add 'a' to set          | {'a'}           | 0      |
   | 2         | b         | Add 'b' to set          | {'a', 'b'}      | 0      |
   | 3         | c         | Add 'c' to set          | {'a', 'b', 'c'} | 0      |
   | 4         | c         | Remove 'c', result += 2 | {'a', 'b'}      | 2      |
   | 5         | c         | Add 'c' to set          | {'a', 'b', 'c'} | 2      |
   | 6         | c         | Remove 'c', result += 2 | {'a', 'b'}      | 4      |
   | 7         | d         | Add 'd' to set          | {'a', 'b', 'd'} | 4      |
   | 8         | d         | Remove 'd', result += 2 | {'a', 'b'}      | 6      |

4. **Final character_set:**
    - After processing all characters, `character_set` contains `{'a', 'b'}`.

5. **Center Character Check:**
    - Since `character_set` is not empty, we can use one of the remaining characters ('a' or 'b') as the center of the
      palindrome.
    - `result` is incremented by 1: `result = 7`

6. **Result Calculation/Return:**
    - The function returns the final `result` value of 7.
      This means the longest possible palindrome we can construct from `"abccccdd"` has a length of 7.
      One possible palindrome is `"dccaccd"`.

---

## June 5 -> 1002. Find Common Characters {collapsible="true" default-state="collapsed"}

Given a string array `words`, return *an array of all characters that show up in all strings within the* `words`
*(including duplicates)*.
You may return the answer in **any order**.

**Example 1:**

**Input:** words = ["bella","label","roller"]
**Output:** ["e","l","l"]

**Example 2:**

**Input: words** = ["cool","lock","cook"]
**Output:** ["c","o"]

**Constraints:**

- `1 <= words.length <= 100`
- `1 <= words[i].length <= 100`
- `words[i]` consists of lowercase English letters.

---

### Approach 1: Iterative Character Counting {id="approach_d3_1" collapsible="true" default-state="expanded"}

```Python
def commonChars1(words: List[str]) -> List[str]:
    """
    Finds all characters that appear in all strings within the input list,
    including duplicates.

    This function iterates through the unique characters of the first word
    and counts their occurrences in all words. It then adds each character
    to the result list as many times as its minimum count across all words.

    The time complexity is O(n * m), where `n` is the number of words and 
    `m` is the average length of the words. This is because we iterate
    through each character of each word once, and the count() method takes
    O(m) time on average. The space complexity is O(k), where `k` is the
    number of unique characters in the first word, as we store counts for
    these characters. Since the alphabet size is fixed, the space complexity
    is constant.
    """
    common_chars = []
    for char in set(words[0]):
        # Count occurrences of the character in each word
        char_counts = [word.count(char) for word in words]
        # Add the character to the result, repeated by its minimum count
        common_chars.extend([char] * min(char_counts))
    return common_chars
```

{collapsible="true" default-state="expanded" collapsed-title="Iterative Character Counting Code..."}

#### Understanding the Core Idea {id="core-idea_d3_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage the set of unique characters from the first word as a reference
point, then count the occurrences of these characters in all words.
By finding the minimum count for each character across all words, we determine how many times that character appears in
every word.

- **Reference Word:** The first word in the list serves as a reference for potential common characters.
- **Character Counting:** For each unique character in the reference word, we count its occurrences in all words.
- **Minimum Frequency:** The minimum count of a character across all words represents how many times it appears in every
  word.

> **Key Insight:**
> By using the set of characters from the first word, we optimize the search space to only relevant
> characters, avoiding unnecessary checks for characters that don't appear in the first word.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d3_1" collapsible="true" default-state="expanded"}

1. **Initialization:**
   ```python
   common_chars = []
   ```
   We initialize an empty list to store the common characters. This will be our final result.

2. **Iterating Through Unique Characters:**
   ```python
   for char in set(words[0]):
   ```
   We iterate through the set of unique characters in the first word.
   This is efficient as we only need to check each unique character once, and it limits our search to characters that
   actually appear in at least one word.

3. **Counting Character Occurrences:**
   ```python
   char_counts = [word.count(char) for word in words]
   ```
   For each character, we create a list comprehension that counts its occurrences in every word.
   This gives us a list of counts for the current character across all words.

4. **Adding Common Characters:**
   ```python
   common_chars.extend([char] * min(char_counts))
   ```
   We add the current character to our result list.
   The number of times we add it is determined by the minimum count across all words.
   This ensures we only include the character as many times as it appears in every word.

5. **Result Return:**
   ```python
   return common_chars
   ```
   After processing all characters, we return the list of common characters.

---

#### Complexity Analysis {id="complexity-analysis_d3_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n \cdot m)$, where $n$ is the number of words and $m$ is the average length of the words.
  This is because for each unique character in the first word (which takes $O(k)$ time, where $k$ is the number of
  unique characters in the first word), we count its occurrences in all $n$ words.
  The `count()` method takes $O(m)$ time on average for each word.
- Therefore, we have $O(k \cdot n \cdot m)$, but since $k$ is bounded by the alphabet size (26 lowercase English
  letters), the time complexity is effectively $O(n \cdot m)$.

**Space Complexity:**

- $O(k)$, where $k$ is the number of unique characters in the first word.
  In the worst case, this is equal to the length of the first word.
  However, since we're dealing with lowercase English letters only, $k$ is bounded by 26, making the
  space complexity effectively $O(1)$ (constant).

---

#### Example {id="example_d3_1" collapsible="true" default-state="expanded"}

**Input:**

```python
words = ['bella', 'label', 'roller']
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - We start by initializing an empty list `common_chars = []` to store the common characters.

2. **Main Loop (Iterating through unique characters of the first word):**

    - The set of unique characters results in `{'e', 'b', 'a', 'l'}` (sets are unordered).
    - **Iteration 1: Character 'e'**
        - We examine the character 'e' from the set of unique characters in 'bella'.
        - We count occurrences of 'e' in each word:
            - 'bella': 1
            - 'label': 1
            - 'roller': 1
        - We calculate the minimum count: min([1, 1, 1]) = 1
        - We add 'e' to `common_chars` once: `common_chars = ['e']`

    - **Iteration 2: Character 'b'**
        - We examine the character 'b'.
        - We count occurrences of 'b' in each word:
            - 'bella': 1
            - 'label': 1
            - 'roller': 0
        - We calculate the minimum count: min([1, 1, 0]) = 0
        - We don't add 'b' to `common_chars` as the minimum count is 0.

    - **Iteration 3: Character 'a'**
        - We examine the character 'a'.
        - We count occurrences of 'a' in each word:
            - 'bella': 1
            - 'label': 1
            - 'roller': 0
        - We calculate the minimum count: min([1, 1, 0]) = 0
        - We don't add 'a' to `common_chars` as the minimum count is 0.

    - **Iteration 4: Character 'l'**
        - We examine the character 'l'.
        - We count occurrences of 'l' in each word:
            - 'bella': 2
            - 'label': 2
            - 'roller': 2
        - We calculate the minimum count: min([2, 2, 2]) = 2
        - We add 'l' to `common_chars` twice: `common_chars = ['e', 'l', 'l']`

3. **Loop Termination:**
    - The loop terminates after we've examined all unique characters in the first word ('bella').
    - At this point, `common_chars` contains ['e', 'l', 'l'].

4. **Visual Aid:**

   Here's a table summarizing the iterations:

   | Iteration | Character | Counts in Words | Min Count | Added to Result |
   |-----------|-----------|-----------------|-----------|-----------------|
   | 1         | 'e'       | [1, 1, 1]       | 1         | ['e']           |
   | 2         | 'b'       | [1, 1, 0]       | 0         | []              |
   | 3         | 'a'       | [1, 1, 0]       | 0         | []              |
   | 4         | 'l'       | [2, 2, 2]       | 2         | ['l', 'l']      |

5. **Result Calculation/Final Steps:**
    - After examining all unique characters in the first word, we have our final result in `common_chars`.
    - The function returns this list: `['e', 'l', 'l']`

---

### Approach 2: Counter Intersection {id="approach_d3_2" collapsible="true" default-state="expanded"}

```Python
def commonChars2(words: List[str]) -> List[str]:
    """
    Finds all characters that appear in all strings within the input list,
    including duplicates.

    This function leverages Python's Counter class to efficiently count
    character occurrences and perform set-like operations. It starts by
    creating a Counter for the first word, then intersects it with Counters
    of subsequent words. 
    
    TThe time complexity is O(n * m), where `n` is the number of words and
    `m` is the average length of the words. This is because creating a
    Counter for each word takes O(m) time on average, and we do this for
    each of the `n` words. The space complexity is O(1) as the size of the
    alphabet is fixed (26 lowercase English letters), so the Counter's size
    is bounded regardless of input size.
    """
    char_counts = Counter(words[0])

    for word in words[1:]:
        # Intersect current counts with counts from the new word
        char_counts &= Counter(word)

    return list(char_counts.elements())
```

{collapsible="true" default-state="expanded" collapsed-title="Counter Intersection Code..."}

#### Understanding the Core Idea {id="core-idea_d3_2" collapsible="true" default-state="expanded"}

The central concept of this solution is to use Python's `Counter` class, which provides an efficient way to count
occurrences of elements and perform set-like operations on these counts.
By intersecting `Counter` objects for each word, we effectively find the common characters and their frequencies across
all words.

- **Counter Objects:** Each word is converted into a `Counter` object, which maps characters to their frequencies.
- **Counter Intersection:** The `&` operation between `Counter` objects keeps the minimum count for each character
  present in both `Counter`s.
- **Iterative Reduction:** By iteratively intersecting `Counter`s for all words, we end up with counts of only the
  common characters.

**Key Insight:** The `Counter` class provides a highly efficient way to perform the intersection operation, which is the
core of finding common elements with their frequencies.

---

#### Code Walkthrough {id="code-walkthrough_d3_2" collapsible="true" default-state="expanded"}

1. **Initial Counter Creation:**
   ```python
   char_counts = Counter(words[0])
   ```
   We create a `Counter` object for the first word. This gives us the character frequencies for our initial reference.

2. **Iterative Counter Intersection:**
   ```python
   for word in words[1:]:
       char_counts &= Counter(word)
   ```
   We iterate through the remaining words, creating a `Counter` for each and intersecting it with our
   running `char_counts`.
   The `&=` operation updates `char_counts` to keep only the characters present in both, with the
   minimum of their counts.

3. **Result Generation and Return:**
   ```python
   return list(char_counts.elements())
   ```
   The `elements()` method of `Counter` returns an iterator over elements repeating all as many times as its count.
   We convert this to a list to get our final result of common characters, including duplicates.

---

#### Complexity Analysis {id="complexity-analysis_d3_2" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n \cdot m)$, where $n$ is the number of words and $m$ is the average length of the words.
  Creating a `Counter` for each word takes $O(m)$ time, and we do this for each of the $n$ words.
  The intersection operation (`&=`) between Counters is typically $O(\min(\text{len}(c_1), \text{len}(c_2)))$,
  where $c_1$ and $c_2$ are the Counters being intersected.
  In our case, this is bounded by the alphabet size (26 lowercase English letters), making the overall time complexity
  $O(n \cdot m)$.

**Space Complexity:**

- $O(1)$ (constant), because we're dealing with a fixed alphabet of lowercase English letters.
  The `Counter` objects will never have more than 26 entries, regardless of the input size.
  Therefore, the space used is bounded by a constant.

---

#### Example {id="example_d3_2" collapsible="true" default-state="expanded"}

**Input:**

```python
words = ['bella', 'label', 'roller']
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - We start by creating a `Counter` object for the first word 'bella':
      `char_counts = Counter('bella') = Counter({'l': 2, 'b': 1, 'e': 1, 'a': 1})`
    - This `Counter` represents the initial common character counts.

2. **Main Loop (Intersecting Character Counts):**

    - **Iteration 1: Comparing with the word 'label'**
        - We create a `Counter` for 'label': `Counter({'l': 2, 'a': 1, 'b': 1, 'e': 1})`
        - We perform an intersection operation (`&=`) between the current `char_counts` and the new `Counter`:
        - The result is: `Counter({'l': 2, 'b': 1, 'e': 1, 'a': 1})`
        - This operation keeps the minimum count for each character present in both Counters.

    - **Iteration 2: Comparing with the word 'roller'**
        - We create a `Counter` for 'roller': `Counter({'r': 2, 'l': 2, 'o': 1, 'e': 1})`
        - We perform another intersection operation with the current `char_counts`:
        - The result is: `Counter({'l': 2, 'e': 1})`
        - Characters 'b' and 'a' are removed as they don't appear in 'roller'.
        - The count for 'l' remains 2 as it appears at least twice in all words so far.

3. **Loop Termination:**
    - The loop terminates after we've processed all words in the input list.
    - At this point, `char_counts` contains `Counter({'l': 2, 'e': 1})`.

4. **Visual Aid:**

   Here's a table summarizing the iterations:

   | Iteration | Word   | Common Chars Counter                        |
            |-----------|--------|---------------------------------------------|
   | Initial   | bella  | `Counter({'l': 2, 'b': 1, 'e': 1, 'a': 1})` |
   | 1         | label  | `Counter({'l': 2, 'b': 1, 'e': 1, 'a': 1})` |
   | 2         | roller | `Counter({'l': 2, 'e': 1})`                 |

5. **Result Calculation/Final Steps:**
    - After processing all words, we have our final `char_counts` Counter.
    - We use the `elements()` method of the Counter to get an iterator of the elements:
      ```python
      char_counts.elements() -> ['e', 'l', 'l']
      ```
    - This iterator repeats each element as many times as its count in the Counter.
    - We convert this iterator to a list: `list(char_counts.elements())`
    - The function returns this list: `['e', 'l', 'l']`

---

## June 6 -> 846. Hand of Straights {collapsible="true" default-state="collapsed"}

Alice has some number of cards, and she wants to rearrange the cards into groups so that each group is of
size `groupSize`, and consists of `groupSize` consecutive cards.

Given an integer array `hand` where `hand[i]` is the value written on the `ith` card and an integer `groupSize`,
return `true` if she can rearrange the cards, or `false` otherwise.

**Example 1:**

- **Input:** hand = [1,2,3,6,2,3,4,7,8], groupSize = 3
- **Output:** true
- **Explanation:** Alice's hand can be rearranged as [1,2,3],[2,3,4],[6,7,8]

**Example 2:**

- **Input:** hand = [1,2,3,4,5], groupSize = 4
- **Output:** false
- **Explanation:** Alice's hand cannot be rearranged into groups of 4.

**Constraints:**

- `1 <= hand.length <= 10^4`
- `0 <= hand[i] <= 10^9`
- `1 <= groupSize <= hand.length`

---

### Approach 1: Greedy Algorithm with Min Heap {id="approach_d4_1" collapsible="true" default-state="expanded"}

```Python
def isNStraightHand1(hand: List[int], group_size: int) -> bool:
    """
    Determines if a list of cards can be arranged into groups of consecutive
    cards of a given size.

    This function uses a greedy approach combined with a min heap to
    efficiently group cards.  It first checks if the total number of cards
    is divisible by the group size. Then, it uses  a Counter to track card
    frequencies and a min heap to process cards in ascending order. The
    algorithm attempts to form groups starting from the smallest available
    card, ensuring consecutive values within each group. This approach is
    optimal as it guarantees the formation of valid groups if they exist,
    while quickly identifying impossible arrangements.

    The time complexity of this solution is O(n log k), where `n` is the
    number of cards and `k` is  the number of unique card values. This is
    due to the heap operations performed for each card. The space complexity
    is O(k) for storing the Counter and min heap. 
    """
    if len(hand) % group_size != 0:
        return False

    card_count = Counter(hand)
    card_min_heap = list(card_count.keys())
    heapq.heapify(card_min_heap)

    while card_min_heap:
        start_card = card_min_heap[0]

        for offset in range(group_size):
            current_card = start_card + offset
            if card_count[current_card] == 0:
                return False
            card_count[current_card] -= 1
            if card_count[current_card] == 0:
                heapq.heappop(card_min_heap)

    return True
```

{collapsible="true" default-state="expanded" collapsed-title="Greedy Algorithm with Min Heap Code..."}

#### Understanding the Core Idea {id="core-idea_d4_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage a greedy algorithm combined with a min heap to efficiently group
consecutive cards.
This approach exploits the ordered nature of the problem and the constraint of forming groups of
consecutive cards.

- **Greedy Strategy:** The solution starts with the smallest card and attempts to form a group of consecutive cards from
  there.
  This greedy approach works because if a valid grouping exists, it must include the smallest card in some group.

- **Min Heap Usage:** A min heap is used to always have quick access to the smallest card value remaining.
  This allows the algorithm to efficiently process cards in ascending order.

- **Counter for Frequency:** A Counter is used to keep track of the frequency of each card value, allowing for efficient
  updates and checks.

> **Key Insight:** By combining the min heap with the Counter, we can efficiently remove cards as we use them in groups,
> and automatically skip over card values that have been completely used up.
> This avoids unnecessary iterations and checks.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d4_1" collapsible="true" default-state="expanded"}

1. **Initial Validation:**
   ```python
   if len(hand) % group_size != 0:
       return False
   ```
   This check ensures that the total number of cards is divisible by the group size.
   If not, it's impossible to form valid groups, so we return `False` immediately.

2. **Initialization of Data Structures:**
   ```python
   card_count = Counter(hand)
   card_min_heap = list(card_count.keys())
   heapq.heapify(card_min_heap)
   ```
   Here, we create a Counter to store the frequency of each card value.
   We then create a min heap (`card_min_heap`) from the unique card values.
   This heap will allow us to efficiently access the smallest card value at any point.

3. **Main Loop - Processing Cards:**
   ```python
   while card_min_heap:
       start_card = card_min_heap[0]
   ```
   We continue processing cards as long as there are cards left in the min heap.
   The smallest card value becomes the start of a potential group.

4. **Group Formation:**
   ```python
   for offset in range(group_size):
       current_card = start_card + offset
       if card_count[current_card] == 0:
           return False
   ```
   We attempt to form a group of `group_size` consecutive cards starting from `start_card`.
   If at any point we find a card that doesn't exist (frequency is 0), we return `False` as we can't form a valid group.

5. **Updating Card Counts:**
   ```python
   card_count[current_card] -= 1
   if card_count[current_card] == 0:
       heapq.heappop(card_min_heap)
   ```
   After using a card in a group, we decrease its count.
   If the count becomes 0, we remove that card value from the min heap as it's no longer available for future groups.

6. **Result:**
   ```python
   return True
   ```
   If we've processed all cards without returning `False`, it means we've successfully grouped all cards, so we
   return `True`.

---

#### Complexity Analysis {id="complexity-analysis_d4_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n \log k)$, where $n$ is the number of cards and $k$ is the number of unique card values. This is because:
    - Creating the Counter and initializing the heap takes $O(n \log k)$ time.
    - The main loop runs at most $n$ times (once for each card).
    - Each iteration of the inner loop is $O(1)$, but may involve a heap operation ($O(\log k)$) if a card count reaches
      zero.
    - Thus, the overall time complexity is dominated by the heap operations, resulting in $O(n \log k)$.

**Space Complexity:**

- $O(k)$, where $k$ is the number of unique card values. This is because:
    - The Counter and the min heap both store at most $k$ elements (one for each unique card value).
    - No other significant extra space is used.
    - While $k$ could potentially be as large as $n$, in practice it's often smaller, especially given the constraint on
      the range of card values.

---

#### Example {id="example_d4_1" collapsible="true" default-state="expanded"}

**Input:**

```python
hand = [1, 2, 3, 6, 2, 3, 4, 7, 8]
group_size = 3
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - First, we check if the length of `hand` (9) is divisible by `group_size` (3).
      9 % 3 = 0, so this condition is satisfied.
    - We create a `Counter` object `card_count` to store the frequency of each card:
      `card_count = {1: 1, 2: 2, 3: 2, 6: 1, 4: 1, 7: 1, 8: 1}`
    - We initialize `card_min_heap` with the unique card values:
      `card_min_heap = [1, 2, 3, 6, 4, 7, 8]`
    - After heapifying, `card_min_heap` remains `[1, 2, 3, 6, 4, 7, 8]` (already in min-heap order)

2. **Main Loop:**

    - **Iteration 1:**
        - `start_card = 1` (the smallest card in the heap)
        - We attempt to form a group of size 3 starting from 1
        - Process cards 1, 2, and 3:
            - Decrement `card_count` for each: `{1: 0, 2: 1, 3: 1, 6: 1, 4: 1, 7: 1, 8: 1}`
            - Remove 1 from `card_min_heap` as its count becomes 0
        - After this iteration: `card_min_heap = [2, 4, 3, 6, 8, 7]`

    - **Iteration 2:**
        - `start_card = 2` (now the smallest card in the heap)
        - We form another group of size 3 starting from 2
        - Process cards 2, 3, and 4:
            - Decrement `card_count` for each: `{1: 0, 2: 0, 3: 0, 6: 1, 4: 0, 7: 1, 8: 1}`
            - Remove 2, 3, and 4 from `card_min_heap` as their counts become 0
        - After this iteration: `card_min_heap = [6, 8, 7]`

    - **Iteration 3:**
        - `start_card = 6` (now the smallest card in the heap)
        - We form the final group of size 3 starting from 6
        - Process cards 6, 7, and 8:
            - Decrement `card_count` for each: `{1: 0, 2: 0, 3: 0, 6: 0, 4: 0, 7: 0, 8: 0}`
            - Remove 6, 7, and 8 from `card_min_heap` as their counts become 0
        - After this iteration: `card_min_heap = []` (empty)

3. **Loop Termination:**
    - The loop terminates as `card_min_heap` is now empty
    - All cards have been successfully grouped into consecutive triples

4. **Visual Aid:**

   Iteration Summary Table:

   | Iteration | Group Size | Card Count                                          | Min Heap           |
   |-----------|------------|-----------------------------------------------------|--------------------|
   | 1         | 3          | Counter({2: 1, 3: 1, 6: 1, 4: 1, 7: 1, 8: 1, 1: 0}) | [2, 4, 3, 6, 8, 7] |
   | 2         | 3          | Counter({6: 1, 7: 1, 8: 1, 1: 0, 2: 0, 3: 0, 4: 0}) | [6, 8, 7]          |
   | 3         | 3          | Counter({1: 0, 2: 0, 3: 0, 6: 0, 4: 0, 7: 0, 8: 0}) | []                 |

5. **Result Calculation:**
    - The algorithm successfully grouped all cards into consecutive triples:
      (1, 2, 3), (2, 3, 4), (6, 7, 8)
    - No cards remain ungrouped, and all groups satisfy the consecutive condition
    - Therefore, the function returns `True`, indicating that it is possible to arrange the cards into groups of size 3
      with consecutive values

---

### Approach 2: Optimized Unsorted Greedy Algorithm {id="approach_d4_2" collapsible="true" default-state="expanded"}

```Python
def isNStraightHand2(hand: List[int], group_size: int) -> bool:
    """
    Determines if a list of cards can be arranged into groups of consecutive
    cards of a given size.

    This function employs a two-pass strategy to efficiently group cards. It
    first checks if the total number of cards is divisible by the group size.
    Then, it uses a Counter to track card frequencies. For each card, it
    finds the start of its group and attempts to remove complete groups. This
    approach is optimized by using helper functions to find group starts and
    remove groups, allowing for cleaner code and potentially better
    performance through local optimizations without needing to sort the cards.

    The time complexity of this solution is O(n), where `n` is the number of
    cards. Despite nested loops, each card is processed at most twice: once
    to find the group start and once to remove it from a group. The space
    complexity is O(k), where `k` is the number of unique card values, used
    for storing the Counter.
    """
    if len(hand) % group_size != 0:
        return False

    card_count = Counter(hand)

    def find_group_start(card):
        """
        Helper function to find the start of a consecutive group for a given
        card.
        """
        group_start = card
        while card_count[group_start - 1]:
            group_start -= 1
        return group_start

    def remove_group(group_start):
        """
        Helper function to remove a group of consecutive cards starting from
        a given card.
        """
        for card in range(group_start, group_start + group_size):
            if not card_count[card]:
                return False
            card_count[card] -= 1
        return True

    for card in hand:
        group_start = find_group_start(card)
        while group_start <= card:
            while card_count[group_start]:
                if not remove_group(group_start):
                    return False
            group_start += 1

    return True
```

{collapsible="true" default-state="expanded" collapsed-title="Approach Name Code..."}

#### Understanding the Core Idea {id="core-idea_d4_2" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage a greedy algorithm that processes cards in their original order,
without the need for sorting.
This approach exploits the problem's nature by forming groups on-the-fly as it iterates through the cards.

- **Unsorted Processing:** Unlike approaches that sort the cards first, this method processes cards in their given
  order, reducing the overhead of sorting.

- **Dynamic Group Formation:** The algorithm dynamically finds and forms groups as it goes, starting from the lowest
  possible card for each group.

- **Frequency Tracking:** A Counter is used to keep track of the frequency of each card value, allowing for efficient
  updates and checks without modifying the original list.

> **Key Insight:**
> By finding the start of a potential group for each card and then attempting to form all possible
> groups from that point, the algorithm ensures it doesn't miss any valid groupings while maintaining linear time
> complexity.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d4_2" collapsible="true" default-state="expanded"}

1. **Initial Validation:**
   ```python
   if len(hand) % group_size != 0:
       return False
   ```
   This check ensures that the total number of cards is divisible by the group size.
   If not, it's impossible to form valid groups, so we return `False` immediately.

2. **Initialization of Data Structure:**
   ```python
   card_count = Counter(hand)
   ```
   Here, we create a Counter to store the frequency of each card value.
   This allows for efficient lookup and modification of card counts.

3. **Helper Function: Finding Group Start:**
   ```python
   def find_group_start(card):
       group_start = card
       while card_count[group_start - 1]:
           group_start -= 1
       return group_start
   ```
   This function finds the start of a potential group for a given card.
   It moves backwards from the current card until it finds a gap in the sequence.
   This is crucial for ensuring we start groups at the earliest possible card, which is necessary for a valid grouping.

4. **Helper Function: Removing a Group:**
   ```python
   def remove_group(group_start):
       for card in range(group_start, group_start + group_size):
           if not card_count[card]:
               return False
           card_count[card] -= 1
       return True
   ```
   This function attempts to remove a group of `group_size` consecutive cards starting from `group_start`.
   It checks if each card in the potential group exists and decrements its count.
   If at any point a required card is missing, it returns `False`, indicating an invalid group.

5. **Main Loop - Processing Cards:**
   ```python
   for card in hand:
       group_start = find_group_start(card)
       while group_start <= card:
           while card_count[group_start]:
               if not remove_group(group_start):
                   return False
           group_start += 1
   ```
   This is the core of the algorithm.
   For each card, it finds the start of its potential group and then attempts to remove all possible groups starting
   from that point up to the current card.
   This ensures that we process all necessary groups for each card while avoiding redundant checks.

6. **Result:**
   ```python
   return True
   ```
   If we've processed all cards without returning `False`, it means we've successfully grouped all cards, so we
   return `True`.

---

#### Complexity Analysis {id="complexity-analysis_d4_2" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n)$, where $n$ is the number of cards. This is because:
    - Each card is processed at most twice: once when finding the group start, and once when removing it as part of a
      group.
    - Although there are nested loops, the total number of iterations across all loops is still proportional to $n$.
    - The use of a Counter allows for O(1) lookups and updates.

**Space Complexity:**

- $O(k)$, where $k$ is the number of unique card values. This is because:
    - The Counter stores at most $k$ elements (one for each unique card value).
    - No other significant extra space is used.
    - While $k$ could potentially be as large as $n$, in practice it's often smaller, especially given the constraint on
      the range of card values.

---

#### Example {id="example_d4_2" collapsible="true" default-state="expanded"}

**Input:**

```python
hand = [1, 2, 3, 6, 2, 3, 4, 7, 8]
group_size = 3
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - First, we check if the length of `hand` (9) is divisible by `group_size` (3).
      9 % 3 = 0, so this condition is satisfied.
    - We create a `Counter` object `card_count` to store the frequency of each card:
      `card_count = {1: 1, 2: 2, 3: 2, 6: 1, 4: 1, 7: 1, 8: 1}`

2. **Main Loop:**
   The function iterates through each card in `hand`. For each card, it finds the start of its group and attempts to
   remove complete groups.

    - **Processing card 1:**
        - `find_group_start(1)` returns 1 (no lower consecutive cards)
        - `group_start = 1`
        - Enter while loop (`group_start (1) <= card (1)`)
            - Enter inner while loop (`card_count[1] = 1`)
                - Attempt to remove group starting at 1
                    - Decrement `card_count[1]` from 1 to 0
                    - Decrement `card_count[2]` from 2 to 1
                    - Decrement `card_count[3]` from 2 to 1
                - Group removed successfully
        - Move `group_start` from 1 to 2

    - **Processing card 2:**
        - `find_group_start(2)` returns 2 (no lower consecutive cards)
        - `group_start = 2`
        - Enter while loop (`group_start (2) <= card (2)`)
            - Enter inner while loop (`card_count[2] = 1`)
                - Attempt to remove group starting at 2
                    - Decrement `card_count[2]` from 1 to 0
                    - Decrement `card_count[3]` from 1 to 0
                    - Decrement `card_count[4]` from 1 to 0
                - Group removed successfully
        - Move `group_start` from 2 to 3

    - **Processing card 3:**
        - `find_group_start(3)` returns 3 (no lower consecutive cards)
        - `group_start = 3`
        - Enter while loop (`group_start (3) <= card (3)`)
        - Move `group_start` from 3 to 4 (no group to remove as `card_count[3] = 0`)

    - **Processing card 6:**
        - `find_group_start(6)` returns 6 (no lower consecutive cards)
        - `group_start = 6`
        - Enter while loop (`group_start (6) <= card (6)`)
            - Enter inner while loop (`card_count[6] = 1`)
                - Attempt to remove group starting at 6
                    - Decrement `card_count[6]` from 1 to 0
                    - Decrement `card_count[7]` from 1 to 0
                    - Decrement `card_count[8]` from 1 to 0
                - Group removed successfully
        - Move `group_start` from 6 to 7

    - **Processing remaining cards (2, 3, 4, 7, 8):**
        - For each of these cards, `find_group_start` is called, but no groups are removed as their counts
          in `card_count` are already 0.

3. **Visual Aid:**

   Iteration Summary Table:

   | Card | Group Start | Card Count After Each Removal                         |
   |------|-------------|-------------------------------------------------------|
   | 1    | 1           | [Counter({2: 1, 3: 1, 6: 1, 4: 1, 7: 1, 8: 1, 1: 0})] |
   | 2    | 2           | [Counter({3: 1, 6: 1, 4: 1, 7: 1, 8: 1, 1: 0, 2: 0})] |
   | 3    | 3           | [Counter({6: 1, 4: 1, 7: 1, 8: 1, 1: 0, 2: 0, 3: 0})] |

4. **Result Calculation:**
    - The algorithm successfully grouped all cards into consecutive triples:
      (1, 2, 3), (2, 3, 4), (6, 7, 8)
    - No cards remain ungrouped, and all groups satisfy the consecutive condition
    - Therefore, the function returns `True`, indicating that it is possible to arrange the cards into groups of size 3
      with consecutive values

---

## June 7 -> 648. Replace Words {collapsible="true" default-state="collapsed"}

In English, we have a concept called **root**, which can be followed by some other word to form another longer word;
let's call this word **derivative**.
For example, when the **root** `"help"` is followed by the word `"ful"`, we can form a derivative `"helpful"`.

Given a `dictionary` consisting of many **roots** and a `sentence` consisting of words separated by spaces, replace all
the derivatives in the sentence with the **root** forming it.
If a derivative can be replaced by more than one **root**,
replace it with the **root** that has **the shortest length**.

Return *the `sentence`* after the replacement.

**Example 1:**

- **Input:** `dictionary = ["cat","bat","rat"]`, `sentence = "the cattle was rattled by the battery"`
- **Output:** `"the cat was rat by the bat"`

**Example 2:**

- **Input:** `dictionary = ["a","b","c"]`, `sentence = "aadsfasf absbs bbab cadsfafs"`
- **Output:** "a a b c"

**Constraints:**

- `1 <= dictionary.length <= 1000`
- `1 <= dictionary[i].length <= 100`
- `dictionary[i]` consists of only lower-case letters.
- `1 <= sentence.length <= 10^6`
- `sentence` consists of only lower-case letters and spaces.
- The number of words in `sentence` is in the range `[1, 1000]`
- The length of each word in `sentence` is in the range `[1, 1000]`
- Every two consecutive words in `sentence` will be separated by exactly one space.
- `sentence` does not have leading or trailing spaces.

---

### Approach 1: String Matching with List Comprehension {id="approach_d5_1" collapsible="true" default-state="expanded"}

```Python
def replaceWords1(dictionary: List[str], sentence: str) -> str:
    """
    Replaces derivative words in a sentence with their shortest root from a
    given dictionary.

    This function uses a list comprehension to iterate through each word in
    the sentence, applying the `find_shortest_root` helper function to each
    word. The core logic relies on checking if each word starts with any of
    the roots in the dictionary, and if so, replacing it with the shortest
    such root. This approach prioritizes simplicity and readability over
    optimal performance for very large inputs.

    The time complexity of this solution is O(n * m * r), where `n` is the
    number of words in the sentence, `m` is the average length of each word,
    and `r` is the number of roots in the dictionary. This is because for
    each word (n), we potentially check against every root (r) using the
    `startswith` method, which in the worst case examines every character of
    the word (m). Sentence splitting contributes O(n * m) to the time
    complexity but it doesn't affect the overall complexity. The space
    complexity is O(n + r), where `n` is the number of words in the sentence
    and `r` is the number of roots in the dictionary. This accounts for the
    space used by the split words list (n), the list comprehension result
    (n), and the potential worst-case size of matching_roots (r) in the
    helper function.
    """
    
    def find_shortest_root(word: str) -> str:
        """Helper function to find the shortest root for a given word."""
        matching_roots = [root for root in dictionary if word.startswith(root)]
        return min(matching_roots, key=len, default=word)
    
    return " ".join([find_shortest_root(word) for word in sentence.split()])
```

{collapsible="true" default-state="expanded" collapsed-title="String Matching with List Comprehension Code..."}

#### Understanding the Core Idea {id="core-idea_d5_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage string matching and list comprehension to efficiently replace
derivative words with their shortest roots.
This approach iterates through each word in the sentence, checking if it
starts with any root from the dictionary, and replacing it with the shortest matching root if found.

- **Dictionary as Root Repository:** The solution uses the given dictionary as a repository of root words, against which
  each word in the sentence is checked.
- **Prefix Matching:** The core technique involves checking if each word starts with any of the roots, effectively
  treating roots as prefixes.
- **Shortest Root Priority:** When multiple roots match a word, the shortest one is chosen, adhering to the problem's
  requirement.
- **List Comprehension for Efficiency:** The solution uses Python's list comprehension to concisely apply the
  root-finding logic to each word in the sentence.

> **Key Insight:**
> The use of the `startswith()` method allows for efficient prefix checking without the need for more
> complex string manipulation or data structures like tries, balancing simplicity and performance for most input sizes.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d5_1" collapsible="true" default-state="expanded"}

1. **Helper Function Definition:**
   ```python
   def find_shortest_root(word: str) -> str:
       matching_roots = [root for root in dictionary if word.startswith(root)]
       return min(matching_roots, key=len, default=word)
   ```
   This inner function finds the shortest root for a given word.
   It uses a list comprehension to collect all matching roots and then returns the shortest one using `min()`
   with a `key` function.
   If no roots match, it returns the original word.

2. **Sentence Processing:**
   ```python
   return " ".join([find_shortest_root(word) for word in sentence.split()])
   ```
   This line does several things:
    - Splits the sentence into words
    - Applies `find_shortest_root` to each word using a list comprehension
    - Joins the resulting words back into a sentence

   This concise approach efficiently processes each word and reconstructs the sentence in a single line of code.

---

#### Complexity Analysis {id="complexity-analysis_d5_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n \cdot m \cdot r)$, where $n$ is the number of words in the sentence, $m$ is the average length of each word,
  and $r$ is the number of roots in the dictionary.

  This is because:
    - For each word in the sentence ($n$)
    - We potentially check against every root in the dictionary ($r$)
    - Using the `startswith` method, which in the worst case examines every character of the word ($m$)

  The sentence splitting contributes $O(n \cdot m)$ to the time complexity, but it doesn't affect the overall
  complexity.

**Space Complexity:**

- $O(n + r)$, where $n$ is the number of words in the sentence and $r$ is the number of roots in the dictionary.

  This accounts for:
    - The space used by the split words list ($n$)
    - The list comprehension result ($n$)
    - The potential worst-case size of `matching_roots` ($r$) in the helper function

---

#### Example {id="example_d5_1" collapsible="true" default-state="expanded"}

**Input:**

```python
dictionary = ['pre', 'post', 'anti', 'pro']
sentence = "The prewar and postwar eras had both prowar and antiwar sentiments"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - The `dictionary` list is initialized with four roots: `['pre', 'post', 'anti', 'pro']`.
    - The `sentence` string is initialized with "The prewar and postwar eras had both prowar and antiwar sentiments".
    - The sentence is split into
      words: `['The', 'prewar', 'and', 'postwar', 'eras', 'had', 'both', 'prowar', 'and', 'antiwar', 'sentiments']`.

2. **Main Loop (List Comprehension):**

   The main processing occurs within a list comprehension that iterates over each word in the sentence.
   For each word, the `find_shortest_root` function is called:

    - **Iteration 1: 'The'**
        - No root in the dictionary matches, so 'The' remains unchanged.

    - **Iteration 2: 'prewar'**
        - Matching roots: `['pre']`
        - Shortest root: 'pre'
        - 'prewar' is replaced with 'pre'

    - **Iteration 3: 'and'**
        - No matching roots, 'and' remains unchanged.

    - **Iteration 4: 'postwar'**
        - Matching roots: `['post']`
        - Shortest root: 'post'
        - 'postwar' is replaced with 'post'

    - **Iteration 5: 'eras'**
        - No matching roots, 'eras' remains unchanged.

    - **Iteration 6: 'had'**
        - No matching roots, 'had' remains unchanged.

    - **Iteration 7: 'both'**
        - No matching roots, 'both' remains unchanged.

    - **Iteration 8: 'prowar'**
        - Matching roots: `['pro']`
        - Shortest root: 'pro'
        - 'prowar' is replaced with 'pro'

    - **Iteration 9: 'and'**
        - No matching roots, 'and' remains unchanged.

    - **Iteration 10: 'antiwar'**
        - Matching roots: `['anti']`
        - Shortest root: 'anti'
        - 'antiwar' is replaced with 'anti'

    - **Iteration 11: 'sentiments'**
        - No matching roots, 'sentiments' remains unchanged.

3. **Loop Termination:**
    - The list comprehension completes after processing all words in the sentence.
    - A new list is created with the processed
      words: `['The', 'pre', 'and', 'post', 'eras', 'had', 'both', 'pro', 'and', 'anti', 'sentiments']`

4. **Visual Aid:**

   Iteration Summary Table:
        
    | Word # | Original Word | Replaced Word | Action    |
    |--------|---------------|---------------|-----------|
    | 1      | The           | The           | Unchanged |
    | 2      | prewar        | pre           | Replaced  |
    | 3      | and           | and           | Unchanged |
    | 4      | postwar       | post          | Replaced  |
    | 5      | eras          | eras          | Unchanged |
    | 6      | had           | had           | Unchanged |
    | 7      | both          | both          | Unchanged |
    | 8      | prowar        | pro           | Replaced  |
    | 9      | and           | and           | Unchanged |
    | 10     | antiwar       | anti          | Replaced  |
    | 11     | sentiments    | sentiments    | Unchanged |

5. **Result Calculation/Final Steps:**
    - The list of processed words is joined back into a string using `" ".join()`.
    - The final replaced sentence is formed: 'The pre and post eras had both pro and anti sentiments'
    - This final string is returned as the result of the `replaceWords1` function.

---

### Approach 2: Trie-based Root Matching {id="approach_d5_2" collapsible="true" default-state="expanded"}

```Python
def replaceWords2(dictionary: List[str], sentence: str) -> str:
    """
    Replaces derivative words in a sentence with their shortest root from a
    given dictionary.

    This function utilizes a Trie data structure to efficiently store and
    search for root words. It first builds a Trie from the dictionary, then
    processes each word in the sentence to find the shortest matching root.
    The Trie-based approach allows for early termination of the search when
    no matching roots are found, and immediate return when the shortest root
    is identified. This strategy is particularly  effective when dealing with
    a large number of roots or long words.

    The time complexity of this solution is O(r * l + n * m), where `r` is
    the number of roots in the dictionary, `l` is the average length of the
    roots, `n` is the number of words in the sentence, `m` is the average
    length of each word in the sentence. This is because building the Trie
    takes O(r * l) time, and processing each word in the sentence takes O(m)
    time in the worst case, repeated for `n` words. Sentence splitting
    contributes O(n * m) to the time complexity as well, but O(2 * n * m) is
    simplified to O(n * m).

    The space complexity is O(r * l + n), where (r * l) represents the space
    used by the Trie (in the worst case, all roots are distinct) and `n`
    represents the space used by the list of split words in the output.
    """

    class TrieNode:
        """Helper class to represent a node in the Trie."""
        def __init__(self):
            self.children = {}
            self.is_word_end = False

    def build_trie(words: List[str]) -> TrieNode:
        """Helper function to build a Trie from a list of words."""
        root = TrieNode()
        for word in words:
            node = root
            for char in word:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.is_word_end = True
        return root

    root = build_trie(dictionary)

    def find_shortest_root(word: str) -> str:
        """
        Helper function to find the shortest matching root for a given word
        using the Trie.
        """
        node = root
        for index, char in enumerate(word):
            if char not in node.children:
                break
            node = node.children[char]
            if node.is_word_end:
                return word[:index + 1]  # Shortest root found
        return word

    return " ".join(find_shortest_root(word) for word in sentence.split())
```

{collapsible="true" default-state="expanded" collapsed-title="Trie-based Root Matching Code..."}

#### Understanding the Core Idea {id="core-idea_d5_2" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage a Trie data structure to efficiently store and search for root
words.
This approach first constructs a Trie from the dictionary of roots, then processes each word in the sentence to
find the shortest matching root.

- **Trie for Efficient Prefix Matching:** The solution uses a Trie (prefix tree) to store all the roots from the
  dictionary.
  This data structure is particularly well-suited for prefix matching operations.
- **Early Termination:** The Trie allows for early termination of the search when no matching roots are found, improving
  efficiency.
- **Immediate Return of Shortest Root:** As soon as a valid root is found in the Trie, it can be immediately returned,
  ensuring the shortest root is always selected.
- **Optimized for Large Dictionaries:** This approach is especially effective when dealing with a large number of roots
  or long words, as the time complexity for searching is independent of the number of roots.

> **Key Insight:**
> The use of a Trie transforms the problem from a linear search through all roots (as in the previous
> approach) to a single path traversal in the Trie, significantly reducing the time complexity for root matching.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d5_2" collapsible="true" default-state="expanded"}

1. **Trie Node Class Definition:**
   ```python
   class TrieNode:
       def __init__(self):
           self.children = {}
           self.is_word_end = False
   ```
   This class defines the structure of each node in the Trie.
   Each node has a dictionary of children (representing later characters)
   and a flag indicating if it's the end of a word.

2. **Trie Construction:**
   ```python
   def build_trie(words: List[str]) -> TrieNode:
       root = TrieNode()
       for word in words:
           node = root
           for char in word:
               if char not in node.children:
                   node.children[char] = TrieNode()
               node = node.children[char]
           node.is_word_end = True
       return root
   
   root = build_trie(dictionary)
   ```
   This function builds the Trie from the given dictionary.
   It iterates through each word, adding nodes for each character and marking the end of words.
   If a character node doesn't exist, it creates a new node.
   If a character node already exists, it moves to that node for the next character.
   The end of the word is marked when the entire word is processed.

3. **Root Finding Function:**
   ```python
   def find_shortest_root(word: str) -> str:
       node = root
       for index, char in enumerate(word):
           if char not in node.children:
               break
           node = node.children[char]
           if node.is_word_end:
               return word[:index + 1]  # Shortest root found
       return word
   ```
   This function traverses the Trie for each word, returning the shortest matching root if found, or the original word
   if no root matches.
   It stops at the first complete word in the Trie, ensuring the shortest root is selected.

4. **Sentence Processing:**
   ```python
   return " ".join(find_shortest_root(word) for word in sentence.split())
   ```
   This line splits the sentence, applies the `find_shortest_root` function to each word using a generator expression,
   and joins the results back into a sentence.

---

#### Complexity Analysis {id="complexity-analysis_d5_2" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(r \times l + n \times m)$, where $r$ is the number of roots in the dictionary, $l$ is the average length of the
  roots, $n$ is the number of words in the sentence, and $m$ is the average length of each word in the sentence.

  This is because:
    - Building the Trie takes $O(r \times l)$ time
    - Processing each word in the sentence takes $O(m)$ time in the worst case
    - This processing is repeated for $n$ words
    - Sentence splitting contributes $O(n \times m)$ to the time complexity

  The total $O(r \times l + 2n \times m)$ simplifies to $O(r \times l + n \times m)$

**Space Complexity:**

- $O(r \times l + n)$, where $r$ is the number of roots in the dictionary, $l$ is the average length of the roots, and
  $n$ is the number of words in the sentence.

  This accounts for:
    - The space used by the Trie $(r \times l)$ in the worst case where all roots are distinct
    - The space used by the list of split words in the output $(n)$

  Note that the actual space used by the Trie might be less than $r \times l$ if there are common prefixes among the
  roots, but we consider the worst-case scenario for complexity analysis.

---

#### Example {id="example_d5_2" collapsible="true" default-state="expanded"}

**Input:**

```python
dictionary = ['pre', 'post', 'anti', 'pro']
sentence = "The prewar and postwar eras had both prowar and antiwar sentiments"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - The `dictionary` list is initialized with four roots: `['pre', 'post', 'anti', 'pro']`.
    - The `sentence` string is initialized with "The prewar and postwar eras had both prowar and antiwar sentiments".
    - The sentence is split into
      words: `['The', 'prewar', 'and', 'postwar', 'eras', 'had', 'both', 'prowar', 'and', 'antiwar', 'sentiments']`.

2. **Building Trie:**
    - A Trie data structure is built using the roots from the dictionary:
        - 'pre': Added to the Trie (new nodes for 'p', 'r', 'e')
        - 'post': Added to the Trie (new nodes for 'o', 's', 't')
        - 'anti': Added to the Trie (new nodes for 'a', 'n', 't', 'i')
        - 'pro': Added to the Trie (new node for 'o' under existing 'p' node)
    - Each word end is marked in the Trie.

    Here is a visual representation of the Trie structure:
    
    ![june07-2024-ap2-trie.png](june07-2024-ap2-trie.png)

3. **Main Processing Loop:**

   The main processing occurs by iterating over each word in the sentence and calling the `find_shortest_root` function:

    - **Word 1: 'The'**
        - Checking character: 'T'
        - No matching child node in Trie
        - Result: Unchanged ('The')

    - **Word 2: 'prewar'**
        - Checking characters: 'p', 'r', 'e'
        - Found shortest root: 'pre'
        - Result: Replaced ('pre')

    - **Word 3: 'and'**
        - Checking character: 'a'
        - No matching child node after 'a'
        - Result: Unchanged ('and')

    - **Word 4: 'postwar'**
        - Checking characters: 'p', 'o', 's', 't'
        - Found shortest root: 'post'
        - Result: Replaced ('post')

    - **Word 5: 'eras'**
        - Checking character: 'e'
        - No matching child node
        - Result: Unchanged ('eras')

    - **Word 6: 'had'**
        - Checking character: 'h'
        - No matching child node
        - Result: Unchanged ('had')

    - **Word 7: 'both'**
        - Checking character: 'b'
        - No matching child node
        - Result: Unchanged ('both')

    - **Word 8: 'prowar'**
        - Checking characters: 'p', 'r', 'o'
        - Found shortest root: 'pro'
        - Result: Replaced ('pro')

    - **Word 9: 'and'**
        - Checking character: 'a'
        - No matching child node after 'a'
        - Result: Unchanged ('and')

    - **Word 10: 'antiwar'**
        - Checking characters: 'a', 'n', 't', 'i'
        - Found shortest root: 'anti'
        - Result: Replaced ('anti')

    - **Word 11: 'sentiments'**
        - Checking character: 's'
        - No matching child node
        - Result: Unchanged ('sentiments')

4. **Visual Aid:**

   Iteration Summary Table:

    | Word # | Original Word | Replaced Word | Action    |
    |--------|---------------|---------------|-----------|
    | 1      | The           | The           | Unchanged |
    | 2      | prewar        | pre           | Replaced  |
    | 3      | and           | and           | Unchanged |
    | 4      | postwar       | post          | Replaced  |
    | 5      | eras          | eras          | Unchanged |
    | 6      | had           | had           | Unchanged |
    | 7      | both          | both          | Unchanged |
    | 8      | prowar        | pro           | Replaced  |
    | 9      | and           | and           | Unchanged |
    | 10     | antiwar       | anti          | Replaced  |
    | 11     | sentiments    | sentiments    | Unchanged |
   
5. **Result Calculation/Final Steps:**
    - The list of processed words is joined back into a string using `" ".join()`.
    - The final replaced sentence is formed: 'The pre and post eras had both pro and anti sentiments'
    - This final string is returned as the result of the `replaceWords2` function.

---

### Approach 3: Length-Based Prefix Matching {id="approach_d5_3" collapsible="true" default-state="expanded"}

```Python
def replaceWords3(dictionary: List[str], sentence: str) -> str:
    """
    Replaces derivative words in a sentence with their shortest root from a
    given dictionary.

    This function optimizes the root-finding process by first creating a
    dictionary of roots with their lengths, then iterating through possible
    prefix lengths for each word. The approach first determines the minimum
    and maximum lengths of roots in the dictionary. Then, for each word in
    the sentence, it checks prefixes of increasing length (starting from the
    minimum root length up to the maximum root length or the word length,
    whichever is smaller). This strategy minimizes unnecessary checks and
    ensures finding the shortest root efficiently.

    The time complexity of this solution is O(r * l + n * m), where `r` is
    the number of roots in the dictionary, `l` is the average length of the
    roots, `n` is the number of words in the sentence, `m` is the average
    length of each word in the sentence. This complexity accounts for
    building the `root_lengths` dictionary (r * l) and splitting the sentence
    (n * m). While we also iterate over a range for each word, in the worst
    case this is similar to iterating over each character (m), so we don't 
    add it separately to the final complexity. The space complexity is
    O(r + n), where `r` represents the space used by the `root_lengths`
    dictionary and `n` represents the space for the `words` list and the
    `replaced_words` list.
    """
    root_lengths = {root: len(root) for root in dictionary}
    min_length = min(root_lengths.values())
    max_length = max(root_lengths.values())

    words = sentence.split()

    replaced_words = []
    for word in words:
        replacement = word

        # Check prefixes up to the minimum of max_length and word length
        for length in range(min_length, min(max_length, len(word)) + 1):
            prefix = word[:length]
            if prefix in root_lengths:
                replacement = prefix
                break
        replaced_words.append(replacement)

    return " ".join(replaced_words)
```

{collapsible="true" default-state="expanded" collapsed-title="Length-Based Prefix Matching Code..."}

#### Understanding the Core Idea {id="core-idea_d5_3" collapsible="true" default-state="expanded"}

The central concept of this solution is to optimize the root-finding process by leveraging knowledge of the minimum and
maximum root lengths in the dictionary.
This approach significantly reduces the number of prefix checks needed for each word.

- **Length-Based Iteration:** By determining the minimum and maximum lengths of roots in the dictionary, the solution
  can efficiently iterate through only the relevant prefix lengths for each word.
- **Dictionary Lookup:** The solution uses a dictionary to store roots, allowing for $O(1)$ lookup of
  root lengths and efficient prefix matching for each word.
- **Early Termination:** The search for each word stops as soon as a matching root is found, ensuring the shortest root
  is always selected.
- **Efficient Range Checking:** By using a range-based iteration, the solution avoids checking prefixes that are
  definitely too short or unnecessarily long.

> **Key Insight:** This approach optimizes the search process by narrowing down the possible root lengths based on the
> dictionary's characteristics, rather than blindly checking all possible prefixes or requiring a complex data
> structure like a Trie.
>
{style="note"}

---

#### Code Walkthrough {id="code-walkthrough_d5_3" collapsible="true" default-state="expanded"}

1. **Root Length Analysis:**
   ```python
   root_lengths = {root: len(root) for root in dictionary}
   min_length = min(root_lengths.values())
   max_length = max(root_lengths.values())
   ```
   This crucial preprocessing step determines the minimum and maximum root lengths in the dictionary.
   This information is key to optimizing the later search process.

2. **Sentence Splitting:**
   ```python
   words = sentence.split()
   ```
   The sentence is split into individual words for processing.

3. **Word Processing Loop:**
   ```python
   replaced_words = []
   for word in words:
       replacement = word
   ```
   This initializes the loop to process each word, starting with the assumption that the word will not be replaced.

4. **Optimized Root Matching:**
   ```python
   for length in range(min_length, min(max_length, len(word)) + 1):
       prefix = word[:length]
       if prefix in root_lengths:
           replacement = prefix
           break
   ```
   This is the core optimization of the algorithm:
    - It only checks prefixes within the range of possible root lengths.
    - It starts from `min_length`, avoiding any unnecessary checks of shorter prefixes.
    - It caps the search at either `max_length` or the word's length, whichever is smaller, preventing overlong prefix
      checks.
    - This targeted approach significantly reduces the number of prefix checks for each word.
    - If a matching root is found in the dictionary, it replaces the word with the root and breaks out of the loop.

5. **Replacement Storage:**
   ```python
   replaced_words.append(replacement)
   ```
   The replacement (either the original word or the found root) is added to the list of replaced words.

6. **Result Construction:**
   ```python
   return " ".join(replaced_words)
   ```
   The processed words are joined back into a sentence and returned.

---

#### Complexity Analysis {id="complexity-analysis_d5_3" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(r \times l + n \times m)$, where $r$ is the number of roots in the dictionary, $l$ is the average length of the
  roots, $n$ is the number of words in the sentence, and $m$ is the average length of each word in the sentence.

  This is because:
    - Creating the `root_lengths` dictionary takes $O(r \times l)$ time
    - Splitting the sentence takes $O(n \times m)$ time
    - For each word (n), we iterate over a range of lengths and perform constant-time operations.
      In the worst case, this is similar to iterating over each character (m).
      In average cases, the range is much smaller than the average word length, so the actual time complexity is lower.
    - The `min()` and `max()` operations on `root_lengths.values()` are $O(r)$, but this is dominated by the $O(r \times
      l)$ of dictionary creation

**Space Complexity:**

- $O(r + n)$, where $r$ is the number of roots in the dictionary and $n$ is the number of words in the sentence.

  This accounts for:
    - The space used by the `root_lengths` dictionary $(r)$
    - The space used by the `words` list $(n)$
    - The space used by the `replaced_words` list $(n)$

  Note that while we create substrings (`prefix`) during processing, these are temporary and don't significantly impact
  the overall space complexity.

This approach offers a good balance between time and space efficiency, particularly effective for scenarios with a large
number of roots or when root lengths vary significantly.

---

#### Example {id="example_d5_3" collapsible="true" default-state="expanded"}

**Input:**

```python
dictionary = ['pre', 'post', 'anti', 'pro']
sentence = "The prewar and postwar eras had both prowar and antiwar sentiments"
```

**Step-by-Step Walkthrough:**

1. **Initialization:**
    - The `dictionary` list is initialized with four roots: `['pre', 'post', 'anti', 'pro']`.
    - The `sentence` string is initialized with "The prewar and postwar eras had both prowar and antiwar sentiments".
    - A `root_lengths` dictionary is created:
           
        | Root | Length |
        |------|--------|
        | pre  | 3      |
        | post | 4      |
        | anti | 4      |
        | pro  | 3      |

    - Minimum root length: 3
    - Maximum root length: 4
    - The sentence is split into
      words: `['The', 'prewar', 'and', 'postwar', 'eras', 'had', 'both', 'prowar', 'and', 'antiwar', 'sentiments']`

2. **Main Processing Loop:**

   The main processing occurs by iterating over each word in the sentence:

    - **Word 1: 'The'**
        - Checking prefix: 'The'
        - No match found
        - Result: Unchanged ('The')

    - **Word 2: 'prewar'**
        - Checking prefix: 'pre'
        - Match found! Replacing with: 'pre'
        - Result: Replaced ('pre')

    - **Word 3: 'and'**
        - Checking prefix: 'and'
        - No match found
        - Result: Unchanged ('and')

    - **Word 4: 'postwar'**
        - Checking prefix: 'pos'
        - No match found
        - Checking prefix: 'post'
        - Match found! Replacing with: 'post'
        - Result: Replaced ('post')

    - **Word 5: 'eras'**
        - Checking prefix: 'era'
        - No match found
        - Checking prefix: 'eras'
        - No match found
        - Result: Unchanged ('eras')

    - **Word 6: 'had'**
        - Checking prefix: 'had'
        - No match found
        - Result: Unchanged ('had')

    - **Word 7: 'both'**
        - Checking prefix: 'bot'
        - No match found
        - Checking prefix: 'both'
        - No match found
        - Result: Unchanged ('both')

    - **Word 8: 'prowar'**
        - Checking prefix: 'pro'
        - Match found! Replacing with: 'pro'
        - Result: Replaced ('pro')

    - **Word 9: 'and'**
        - Checking prefix: 'and'
        - No match found
        - Result: Unchanged ('and')

    - **Word 10: 'antiwar'**
        - Checking prefix: 'ant'
        - No match found
        - Checking prefix: 'anti'
        - Match found! Replacing with: 'anti'
        - Result: Replaced ('anti')

    - **Word 11: 'sentiments'**
        - Checking prefix: 'sen'
        - No match found
        - Checking prefix: 'sent'
        - No match found
        - Result: Unchanged ('sentiments')

3. **Visual Aid:**

   | Word # | Original Word | Replaced Word | Prefixes Checked | Action    |
   |--------|---------------|---------------|------------------|-----------|
   | 1      | The           | The           | The              | Unchanged |
   | 2      | prewar        | pre           | pre              | Replaced  |
   | 3      | and           | and           | and              | Unchanged |
   | 4      | postwar       | post          | pos, post        | Replaced  |
   | 5      | eras          | eras          | era, eras        | Unchanged |
   | 6      | had           | had           | had              | Unchanged |
   | 7      | both          | both          | bot, both        | Unchanged |
   | 8      | prowar        | pro           | pro              | Replaced  |
   | 9      | and           | and           | and              | Unchanged |
   | 10     | antiwar       | anti          | ant, anti        | Replaced  |
   | 11     | sentiments    | sentiments    | sen, sent        | Unchanged |

4. **Result Calculation/Final Steps:**
    - The list of processed words is joined back into a string using `" ".join()`.
    - The final replaced sentence is formed: 'The pre and post eras had both pro and anti sentiments'
    - This final string is returned as the result of the `replaceWords3` function.

---

## June 8 -> 523. Continuous Subarray Sum {collapsible="true" default-state="collapsed"}

Given an integer array nums and an integer k, return `true` *if* `nums` *has a **good subarray** or* `false`
*otherwise*.

A **good subarray** is a subarray where:

- its length is **at least two**, and
- the sum of the elements of the subarray is a multiple of `k`.

**Note** that:

- A **subarray** is a contiguous part of the array.
- An integer `x` is a multiple of `k` if there exists an integer `n` such that `x = n * k`. `0` is **always** a multiple
  of `k`.

**Example 1:**

- **Input:** nums = [23,2,4,6,7], k = 6
- **Output:** true
- **Explanation:** [2, 4] is a continuous subarray of size 2 whose elements sum up to 6.

**Example 2:**

- **Input:** nums = [23,2,6,4,7], k = 6
- **Output:** true
- **Explanation:** [23, 2, 6, 4, 7] is a continuous subarray of size 5 whose elements sum up to 42.
    - 42 is a multiple of 6 because 42 = 7 * 6 and 7 is an integer.

**Example 3:**

- **Input:** nums = [23,2,6,4,7], k = 13
- **Output:** false

**Constraints:**

- `1 <= nums.length <= 10^5`
- `0 <= nums[i] <= 10^9`
- `0 <= sum(nums[i]) <= 2^31 - 1`
- `1 <= k <= 2^31 - 1`

---

### Approach 1: Prefix Sum with Modular Arithmetic {id="approach_d6_1" collapsible="true" default-state="expanded"}

```Python
def checkSubarraySum1(nums: List[int], k: int) -> bool:
    """
    Determines if there exists a subarray in 'nums' with a length of at least
    two and a sum that is a multiple of 'k'.

    This function uses a prefix sum technique combined with modular
    arithmetic to efficiently solve the problem. It iterates through the
    array once, maintaining a running sum modulo `k`. The key insight is that
    if the same remainder is encountered twice, and the indices are at least
    2 apart, it implies the existence of a subarray whose sum is divisible by
    `k`. This approach avoids the need to check all possible subarrays,
    significantly reducing time complexity.

    The time complexity of this solution is O(n), where `n` is the length of
    `nums`, because it performs a single pass through the array. The space
    complexity is O(min(n, k)) as the `remainder_index_map` can have at most
    min(n, k) entries.

    """
    prefix_sum_mod_k = 0
    # Initialize with 0 at index -1 to handle edge cases
    remainder_index_map = {0: -1}

    for index, num in enumerate(nums):
        prefix_sum_mod_k = (prefix_sum_mod_k + num) % k

        if prefix_sum_mod_k not in remainder_index_map:
            remainder_index_map[prefix_sum_mod_k] = index
        elif index - remainder_index_map[prefix_sum_mod_k] >= 2:
            return True  # Valid subarray found

    return False  # No valid subarray found
```

{collapsible="true" default-state="expanded" collapsed-title="Prefix Sum with Modular Arithmetic Code..."}

#### Understanding the Core Idea {id="core-idea_d6_1" collapsible="true" default-state="expanded"}

The central concept of this solution is to leverage prefix sums and modular arithmetic to efficiently identify subarrays
with sums divisible by `k`.
This approach allows us to solve the problem in a single pass through the array, without
needing to check all possible subarrays explicitly.

- **Prefix Sum Property:** If the difference between two prefix sums is divisible by `k`, then the subarray between
  those two points has a sum divisible by `k`.
- **Modular Arithmetic:** By taking the modulus of the running sum at each step, we can identify when we've found a
  subarray with a sum divisible by `k`.
- **Hash Map for Remainders:** Storing the first occurrence of each remainder allows us to quickly check if we've seen
  the same remainder before, indicating a valid subarray.

> **Key Insight:**
> The crucial observation is that if we encounter the same remainder twice, the subarray between these
> two points must have a sum divisible by `k`.
> This is because `(sum[j] - sum[i]) % k == 0` implies that the subarray
> from `i+1` to `j` has a sum divisible by `k`.

---

#### Code Walkthrough {id="code-walkthrough_d6_1" collapsible="true" default-state="expanded"}

1. **Initialization:**
   ```python
   prefix_sum_mod_k = 0
   remainder_index_map = {0: -1}
   ```
   We initialize `prefix_sum_mod_k` to track the running sum modulo `k`.
   The `remainder_index_map` is initialized with `{0: -1}` to handle the case where the entire prefix sum is
   divisible by `k` from the start.
    - We're essentially saying that we've seen a remainder of 0 at an imaginary index -1.
      This clever trick allows us to correctly identify subarrays that start from the beginning of the input array and
      have a sum divisible by `k`.

2. **Iterating Through the Array:**
   ```python
   for index, num in enumerate(nums):
   ```
   We iterate through the array, keeping track of both the index and the number.

3. **Updating Prefix Sum:**
   ```python
   prefix_sum_mod_k = (prefix_sum_mod_k + num) % k
   ```
   We update the running sum and take the modulus with `k`.
   This keeps our sum in the range `[0, k-1]`, allowing us to detect cycles efficiently.

4. **Checking and Updating Remainder Map:**
   ```python
   if prefix_sum_mod_k not in remainder_index_map:
       remainder_index_map[prefix_sum_mod_k] = index
   elif index - remainder_index_map[prefix_sum_mod_k] >= 2:
       return True
   ```
   If we haven't seen this remainder before, we add it to the map.
   If we have seen it before, we check if the subarray length is at least 2. If so,
   we've found a valid subarray and return `True`.

5. **Result Calculation/Return:**
   ```python
   return False
   ```
   If we've gone through the entire array without finding a valid subarray, we return `False`.

---

#### Complexity Analysis {id="complexity-analysis_d6_1" collapsible="true" default-state="expanded"}

**Time Complexity:**

- $O(n)$, where $n$ is the length of the input array `nums`.
  This is because we iterate through the array exactly once, and all operations within the loop
  (modulus calculation, dictionary lookups, and updates) are constant time operations.

**Space Complexity:**

- $O(\min(n, k))$, where $n$ is the length of the input array `nums` and $k$ is the divisor.
  This is because the `remainder_index_map` stores remainders modulo `k`, so there can be at most `k` unique remainders.
  However, if $n \leq k$, we won't encounter more than `n` unique remainders.
  Therefore, the space used is bounded by the minimum of `n` and `k`.

---

#### Example {id="example_d6_1" collapsible="true" default-state="expanded"}

...

---

## June 9 -> 974. Subarray Sums Divisible by K {collapsible="true" default-state="collapsed"}

Given an integer array `nums` and an integer `k`, return *the number of non-empty **subarrays** that have a sum
divisible by* `k`.

A **subarray** is a **contiguous** part of an array.

**Example 1:**

- **Input:** nums = [4,5,0,-2,-3,1], k = 5
- **Output:** 7
- **Explanation:** There are seven subarrays with a sum divisible by k = 5:
  [4, 5, 0, -2, -3, 1], [5], [5, 0], [5, 0, -2, -3], [0], [0, -2, -3], [-2, -3]

**Example 2:**

- **Input:** nums = [5], k = 9
- **Output:** 0

**Constraints:**

- `1 <= nums.length <= 3 * 10^4`
- `-10^4 <= nums[i] <= 10^4`
- `2 <= k <= 10^4`

---

### Approach 1: Approach Name {id="approach_d7_1" collapsible="true" default-state="expanded"}

```Python
def subarraysDivByK1(nums: List[int], k: int) -> int:
    """
    Counts the number of contiguous subarrays in a given list whose sum is divisible by k.

    The function uses a hashmap to keep track of the frequencies of remainders of the cumulative sums when divided by k.
    By maintaining the count of these remainders, 
    it efficiently counts the number of subarrays that have a sum divisible by k.
    The key insight is that if two prefix sums have the same remainder when divided by k, their difference is a 
    multiple of k, which implies that the subarray between these two sums has a sum divisible by k.

    The time complexity of this solution is O(n), where n is the length of the input array.
    This is because each element in the array is processed exactly once.
    The space complexity is O(min(n, k)) for the dictionary as it can store at most k remainders.
    If k is larger than n, it stores at most n remainders.
    """
    remainder_counts = {0: 1}
    prefix_sum = 0
    subarray_count = 0

    for num in nums:
        prefix_sum += num
        remainder = prefix_sum % k

        if remainder in remainder_counts:
            subarray_count += remainder_counts[remainder]
            remainder_counts[remainder] += 1
        else:
            remainder_counts[remainder] = 1

    return subarray_count
```

{collapsible="true" default-state="expanded" collapsed-title="Approach Name Code..."}

#### Understanding the Core Idea {id="core-idea_d7_1" collapsible="true" default-state="expanded"}

...

---

#### Code Walkthrough {id="code-walkthrough_d7_1" collapsible="true" default-state="expanded"}

...

---

#### Complexity Analysis {id="complexity-analysis_d7_1" collapsible="true" default-state="expanded"}

...

---

#### Example {id="example_d7_1" collapsible="true" default-state="expanded"}

...

---
